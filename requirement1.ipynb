{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import urllib3\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from nltk.stem.porter import *\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nltk.download('punkt')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('datasets/q1/train.csv', sep=',')\n",
    "test_data = pd.read_csv('datasets/q1/test_without_labels.csv', sep=',')\n",
    "#test_data['Label'] = train_data['Label']\n",
    "expected_array = np.array(train_data.head(2000)[['Label']]).flatten()\n",
    "content_array = np.array(train_data.head(2000)[['Content']]).flatten()\n",
    "#X_train = train_data['Content'].head(2000)\n",
    "#Y_train = train_data['Label'].head(2000)\n",
    "#X_test = test_data['Content']\n",
    "#Y_test = test_data['Label']\n",
    "\n",
    "my_additional_stop_words = ['said','still','day','will','new','may','two','one','now','time','say','second','month','first','going','year','back','people','case','according']\n",
    "stop_words = STOPWORDS.union(my_additional_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b9f2419b00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Label',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Id  Title  Content\n",
      "Label                               \n",
      "Business       24834  24834    24834\n",
      "Entertainment  44834  44834    44834\n",
      "Health         12020  12020    12020\n",
      "Technology     30107  30107    30107\n"
     ]
    }
   ],
   "source": [
    "traindata = train_data.groupby('Label').count()\n",
    "print(traindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entertainment = train_data[train_data['Label']=='Entertainment'].head(3900)\n",
    "#df_health = train_data[train_data['Label']=='Health'].head(1000)\n",
    "df_health = train_data[train_data['Label']=='Health'].head(1200)\n",
    "df_business = train_data[train_data['Label']=='Business'].head(2700)\n",
    "#df_technology = train_data[train_data['Label']=='Technology'].head(2900)\n",
    "df_technology = train_data[train_data['Label']=='Technology'].head(3000)\n",
    "df = pd.concat([df_entertainment,df_health,df_business,df_technology])\n",
    "\n",
    "#X_train = df['Content']\n",
    "#Y_train = df['Label']\n",
    "#X_test = df['Content']\n",
    "#Y_test = test_data['Label']\n",
    "\n",
    "X = df['Title']\n",
    "y = df['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "set(df['Label'])\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['Label'])\n",
    "y = le.transform(df['Label'])\n",
    "set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-6-20c09ce7e3a9>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-20c09ce7e3a9>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    business = str()\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "    #STOPWORDS.extend(['will', 'said', 'us', 'may', 'share','well'])\n",
    "def wordcloud():\n",
    "    business = str()\n",
    "    entertainment = str()\n",
    "    health = str()\n",
    "    technology = str()\n",
    "    for i in range(expected_array.shape[0]):\n",
    "        if expected_array[i] == 'Business':\n",
    "            business = business + ' ' + str(content_array[i])\n",
    "        elif expected_array[i] == 'Entertainment':\n",
    "            entertainment = entertainment + ' ' + str(content_array[i])\n",
    "        elif expected_array[i] == 'Health':\n",
    "            health = health + ' ' + str(content_array[i])\n",
    "        elif expected_array[i] == 'Technology':\n",
    "            technology = technology + ' ' + str(content_array[i])\n",
    "\n",
    "    mask = np.array(Image.open(requests.get('http://www.clker.com/cliparts/3/0/a/1/11971238241310805346noonespillow_Simple_Cloud.svg.med.png',stream=True).raw))\n",
    "\n",
    "    wordcloud1 = WordCloud(stopwords=stop_words, background_color=\"white\", mask=mask).generate(business)\n",
    "    plt.imshow(wordcloud1,interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('q1/business.png')\n",
    "\n",
    "    wordcloud2 = WordCloud(stopwords=stop_words,background_color=\"white\",mask=mask).generate(entertainment)\n",
    "    plt.imshow(wordcloud2,interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('q1/entertainment.png')\n",
    "\n",
    "    wordcloud3 = WordCloud(stopwords=stop_words,background_color=\"white\",mask=mask).generate(health)\n",
    "    plt.imshow(wordcloud3,interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('q1/health.png')\n",
    "\n",
    "    wordcloud4 = WordCloud(stopwords=stop_words,background_color=\"white\",mask=mask).generate(technology)\n",
    "    plt.imshow(wordcloud4,interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('q1/technology.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tfidf\n",
      "Building SVD\n"
     ]
    }
   ],
   "source": [
    "print(\"building tfidf\")\n",
    "\n",
    "#vectorizer = TfidfVectorizer(stop_words = 'english', max_df=0.9, min_df=3)\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', use_idf=True)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) \n",
    "X_test_vect = vectorizer.transform(X_test) \n",
    "\n",
    "print(\"Building SVD\")\n",
    "#truncated svd\n",
    "lsa=TruncatedSVD(n_components = 80)\n",
    "svd_transformer = make_pipeline(vectorizer,lsa)\n",
    "X_train_svd = svd_transformer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before determing model\n",
      "before feeding data\n",
      "before predicting\n",
      "accuracy: 0.8883116883116883\n",
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Business       0.83      0.85      0.84       859\n",
      "Entertainment       0.92      0.95      0.94      1312\n",
      "       Health       0.89      0.79      0.84       338\n",
      "   Technology       0.90      0.87      0.88       956\n",
      "\n",
      "     accuracy                           0.89      3465\n",
      "    macro avg       0.88      0.86      0.87      3465\n",
      " weighted avg       0.89      0.89      0.89      3465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#set candidate parameters\n",
    "parameters = {'kernel':['rbf'], 'C': [4,8,16], 'gamma': [0.1,0.01]}\n",
    "\n",
    "print(\"before determing model\")\n",
    "\n",
    "#determine the model\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc,parameters,cv=5)\n",
    "\n",
    "print(\"before feeding data\")\n",
    "# Feed the training data\n",
    "clf.fit(X_train_tfidf, y_train)  \n",
    "\n",
    "print(\"before predicting\")\n",
    "predictions = clf.predict(X_test_vect)\n",
    "#preds = le.inverse_transform(predictions )\n",
    "\n",
    "print(\"accuracy:\",end=' ')\n",
    "print(accuracy_score(y_test,predictions))\n",
    "print(\"\\n\")\n",
    "print(metrics.classification_report(y_test,predictions))\n",
    "\n",
    "dfr = classification_report(y_test,predictions,output_dict = True)\n",
    "macro_precision = dfr['macro avg']['precision']\n",
    "macro_recall = dfr['macro avg']['recall']\n",
    "macro_f1 = dfr['macro avg']['f1-score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6955238095238095\n",
      "accuracy: 0.6955238095238095\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.65      2700\n",
      "           1       0.64      0.93      0.76      3900\n",
      "           2       0.80      0.38      0.51      1000\n",
      "           3       0.87      0.54      0.67      2900\n",
      "\n",
      "    accuracy                           0.70     10500\n",
      "   macro avg       0.75      0.62      0.65     10500\n",
      "weighted avg       0.73      0.70      0.68     10500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters2 = {'kernel':['rbf'], 'C': [4,8,16], 'gamma': [0.1,0.01]}\n",
    "s2 = SVC()\n",
    "s = GridSearchCV(s2,parameters2, cv=5)\n",
    "\n",
    "#scores=cross_val_score(s,X_train_svd,y,cv=5,scoring='accuracy')\n",
    "#score= scores.mean()\n",
    "#print (score)\n",
    "predicted = cross_val_predict(s, X_train_svd, y, cv=5) #5-fold cross validation\n",
    "print (accuracy_score(y, predicted)) \n",
    "\n",
    "print(\"accuracy:\",end=' ')\n",
    "print(metrics.accuracy_score(y,predicted))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(classification_report(y,predicted))\n",
    "\n",
    "dfr2 = classification_report(y,predicted,output_dict = True)\n",
    "macro_precision2 = dfr2['macro avg']['precision']\n",
    "macro_recall2 = dfr2['macro avg']['recall']\n",
    "macro_f12 = dfr2['macro avg']['f1-score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests (Bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8216450216450216\n",
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Business       0.83      0.71      0.77       859\n",
      "Entertainment       0.77      0.96      0.85      1312\n",
      "       Health       0.87      0.67      0.76       338\n",
      "   Technology       0.90      0.79      0.84       956\n",
      "\n",
      "     accuracy                           0.82      3465\n",
      "    macro avg       0.84      0.78      0.80      3465\n",
      " weighted avg       0.83      0.82      0.82      3465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc1 = RandomForestClassifier(n_estimators=100)\n",
    "params = {'n_estimators':[100]}\n",
    "rfc = GridSearchCV(rfc1,params, cv=5)\n",
    "rfc.fit(X_train_tfidf, y_train)\n",
    "rfc_pred = rfc.predict(X_test_vect)\n",
    "\n",
    "print(\"accuracy:\",end=' ')\n",
    "print(accuracy_score(y_test,rfc_pred))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(metrics.classification_report(y_test,rfc_pred))\n",
    "\n",
    "dfr3 = classification_report(y_test,rfc_pred,output_dict = True)\n",
    "macro_precision3 = dfr3['macro avg']['precision']\n",
    "macro_recall3 = dfr3['macro avg']['recall']\n",
    "macro_f13 = dfr3['macro avg']['f1-score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests (svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.782952380952381\n",
      "\n",
      "\n",
      "0.782952380952381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75      2700\n",
      "           1       0.80      0.90      0.85      3900\n",
      "           2       0.82      0.52      0.64      1000\n",
      "           3       0.82      0.72      0.76      2900\n",
      "\n",
      "    accuracy                           0.78     10500\n",
      "   macro avg       0.79      0.73      0.75     10500\n",
      "weighted avg       0.79      0.78      0.78     10500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestClassifier(n_estimators = 100)\n",
    "#scores=cross_val_score(m,X,y,cv=5,scoring='accuracy')\n",
    "\n",
    "r_predicted = cross_val_predict(m, X_train_svd, y, cv=5) #5-fold cross validation\n",
    "print(\"accuracy:\",end=' ')\n",
    "print(accuracy_score(y,r_predicted))\n",
    "print(\"\\n\")\n",
    "print (accuracy_score(y, r_predicted))\n",
    "print(metrics.classification_report(y,r_predicted))\n",
    "\n",
    "dfr4 = classification_report(y,r_predicted,output_dict = True)\n",
    "macro_precision4 = dfr4['macro avg']['precision']\n",
    "macro_recall4 = dfr4['macro avg']['recall']\n",
    "macro_f14 = dfr4['macro avg']['f1-score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_id = test_data['Id'].tolist()\n",
    "#clue = train_data.loc[train_data['Id'].isin(test_id)]\n",
    "#test_id = pd.Series(test_data['Id'])\n",
    "#td = train_data[train_data['Id'].isin([i])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\python\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['accord', 'arent', 'couldnt', 'didnt', 'doesnt', 'dont', 'els', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'henc', 'howev', 'id', 'ill', 'im', 'isnt', 'ive', 'mustnt', 'otherwis', 'shant', 'shed', 'shell', 'shes', 'shouldnt', 'sinc', 'therefor', 'theyd', 'theyll', 'theyr', 'theyv', 'wasnt', 'wed', 'werent', 'weve', 'wont', 'wouldnt', 'youd', 'youll', 'youv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopWords = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "\n",
    "stopWords.extend(['saying', 'said', 'say', 'yes', 'instead', 'meanwhile', 'right', 'really', 'finally', 'now', \n",
    "                       'one', 'suggested', 'says', 'added', 'think', 'know', 'though', 'let', 'going', 'back',\n",
    "                       'well', 'example', 'us', 'yet', 'perhaps', 'actually', 'oh', 'year', 'lastyear',\n",
    "                       'last', 'old', 'first', 'good', 'maybe', 'ask', '.', ',', ':', 'take', 'made', 'n\\'t', 'go', \n",
    "                       'make', 'two', 'got', 'took', 'want', 'much', 'may', 'never', 'second', 'could', 'still', 'get', \n",
    "                       '?', 'would', '(', '\\'', ')', '``', '/', \"''\", '%', '#', '!', 'next', \"'s\", ';', '[', ']', '...',\n",
    "                       'might', \"'m\", \"'d\", 'also', 'something', 'even', 'new', 'lot', 'a', 'thing', 'time', 'way',\n",
    "                       'always', 'whose', 'need', 'people', 'come', 'become', 'another', 'many', 'must', 'too', 'as', 'well'])\n",
    "\n",
    "stopWords.extend([' ','are','and','I','A','And','So','arnt','This','When','It','many','Many','so','cant','Yes','yes','No','no','These','these'])\n",
    "\n",
    "def processText(text):\n",
    "    #text = text.lower()\n",
    "    #text = re.sub(\"\\'s\", \" \", text)\n",
    "    #text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    #text = re.sub(\"cannot\", \"can't\", text)\n",
    "    #text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
    "    #text = re.sub(\"n't\", \" not \", text)\n",
    "    #text = re.sub(\"\\'re\", \" are \", text)\n",
    "    #text = re.sub(\"\\'d\", \" would \", text)\n",
    "    #text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    #text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n",
    "    #text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
    "    #text = re.sub('\\&', \" and \", text)\n",
    "    \n",
    "    p_stemmer = SnowballStemmer(language='english')\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    tokens = text.lower()\n",
    "    tokens = tokens.split()\n",
    "    tokens = [wrd for wrd in tokens if(wrd not in stopWords)]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [wrd for wrd in tokens if(wrd not in stopWords)]\n",
    "    stems = [p_stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "X = 'ttt '+ df['Title']+' ttt '+ df['Title']+' ccc '+df['Content']+' ttt '+df['Title']+' ttt '+ df['Title']\n",
    "y = df['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=processText, stop_words = stop_words, use_idf=True)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) \n",
    "X_test_vect = vectorizer.transform(X_test) \n",
    "\n",
    "#params2 = {'alpha':[1], 'norm':[True]}\n",
    "params2 = {'kernel':['rbf'], 'C': [8], 'gamma': [0.1]}\n",
    "\n",
    "#nb2 = ComplementNB(alpha=1, norm=True)\n",
    "nb2 = SVC()\n",
    "nb = GridSearchCV(nb2,params2,cv=5)\n",
    "\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "pred5 = nb.predict(X_test_vect)\n",
    "\n",
    "print(\"accuracy:\",end=' ')\n",
    "print(accuracy_score(y_test,pred5))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test,pred5))\n",
    "\n",
    "dfr5 = classification_report(y_test,pred5,output_dict = True)\n",
    "macro_precision5 = dfr5['macro avg']['precision']\n",
    "macro_recall5 = dfr5['macro avg']['recall']\n",
    "macro_f15 = dfr5['macro avg']['f1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = 'ttt '+ test_data['Title']+' ttt '+ test_data['Title']+' ccc '+test_data['Content']+' ttt '+test_data['Title']+' ttt '+ test_data['Title']\n",
    "X_test_vector = vectorizer.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final = nb.predict(X_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_print_arr = np.array(test_data[['Id']]).flatten()\n",
    "columns = pd.Index(['Id','Predicted'])\n",
    "data = np.column_stack((to_print_arr,pred_final))\n",
    "df_final = pd.DataFrame(data, index=None, columns = columns)\n",
    "df_final.to_csv('q1/testSet_categories.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "name=['Id','Predicted']\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def benchmark2(df, df_test):\n",
    "    df['Content'] = df['Content'].str.replace('[^\\w\\s]','')\n",
    "    df['Content'] = df['Content'].str.lower()\n",
    "\n",
    "    df['Title'] = df['Title'].str.replace('[^\\w\\s]','')\n",
    "    df['Title'] = df['Title'].str.lower()\n",
    "\n",
    "    df_test['Content'] = df_test['Content'].str.replace('[^\\w\\s]','')\n",
    "    df_test['Content'] = df_test['Content'].str.lower()\n",
    "\n",
    "    df_test['Title'] = df_test['Title'].str.replace('[^\\w\\s]','')\n",
    "    df_test['Title'] = df_test['Title'].str.lower()\n",
    "\n",
    "    df['Text'] = df['Title'] + ' ' + df['Content']\n",
    "    X_train=df['Text']\n",
    "\n",
    "    df_test['Text'] = df_test['Title'] + ' ' + df_test['Content']\n",
    "    X_test=df_test['Text']\n",
    "\n",
    "    column0=df_test['Id']\n",
    "    print (len(column0))\n",
    "\n",
    "    le=LabelEncoder()\n",
    "    Y_train=le.fit_transform(df['Label'])\n",
    "\n",
    "    svd=TruncatedSVD(n_components=40)\n",
    "    stemmed_count_vect = StemmedCountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "    #clf=RandomForestClassifier()\n",
    "    clf = SVC(kernel='linear')\n",
    "    pipeline = Pipeline([('vect',stemmed_count_vect),('tfidf', TfidfTransformer()),('svd',svd),('clf', clf)])\n",
    "        #Pipeline Fit\n",
    "    pipeline.fit(X_train,Y_train)\n",
    "        #Predict the train set\n",
    "    predicted=pipeline.predict(X_test)\n",
    "\n",
    "    column1=le.inverse_transform(predicted)\n",
    "    print (len(column1))\n",
    "\n",
    "    to_print_arr = np.array(df_test[['Id']]).flatten()\n",
    "    columns = pd.Index(['Id','Predicted'])\n",
    "    data = np.column_stack((to_print_arr,column1))\n",
    "    df_final = pd.DataFrame(data, index=None, columns = columns)\n",
    "    df_final.to_csv('q1/testSet_categories.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "c:\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "c:\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "c:\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47912\n",
      "47912\n"
     ]
    }
   ],
   "source": [
    "#df_b = train_data.head(10000)\n",
    "#df_b_t = test_data\n",
    "#benchmark2(df_b,df_b_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement 1 complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
