{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import urllib3\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from nltk.stem.porter import *\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nltk.download('punkt')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('datasets/q1/train.csv', sep=',')\n",
    "test_data = pd.read_csv('datasets/q1/test_without_labels.csv', sep=',')\n",
    "#test_data['Label'] = train_data['Label']\n",
    "expected_array = np.array(train_data.head(2000)[['Label']]).flatten()\n",
    "content_array = np.array(train_data.head(2000)[['Content']]).flatten()\n",
    "#X_train = train_data['Content'].head(2000)\n",
    "#Y_train = train_data['Label'].head(2000)\n",
    "#X_test = test_data['Content']\n",
    "#Y_test = test_data['Label']\n",
    "\n",
    "my_additional_stop_words = ['said','still','day','will','new','may','two','one','now','time','say','second','month','first','going','year','back','people','case','according']\n",
    "stop_words = STOPWORDS.union(my_additional_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e9991f2358>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Label',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Id  Title  Content\n",
      "Label                               \n",
      "Business       24834  24834    24834\n",
      "Entertainment  44834  44834    44834\n",
      "Health         12020  12020    12020\n",
      "Technology     30107  30107    30107\n"
     ]
    }
   ],
   "source": [
    "traindata = train_data.groupby('Label').count()\n",
    "print(traindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_entertainment = train_data[train_data['Label']=='Entertainment'].head(3900)\n",
    "#df_health = train_data[train_data['Label']=='Health'].head(1200)\n",
    "#df_business = train_data[train_data['Label']=='Business'].head(2700)\n",
    "#df_technology = train_data[train_data['Label']=='Technology'].head(3000)\n",
    "#df = pd.concat([df_entertainment,df_health,df_business,df_technology])\n",
    "\n",
    "df = train_data.head(10000)\n",
    "X = df['Title'] + df['Content']\n",
    "y = df['Label']\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOPWORDS.extend(['will', 'said', 'us', 'may', 'share','well'])\n",
    "def wordcloud():\n",
    "    business = str()\n",
    "    entertainment = str()\n",
    "    health = str()\n",
    "    technology = str()\n",
    "    for i in range(expected_array.shape[0]):\n",
    "        if expected_array[i] == 'Business':\n",
    "            business = business + ' ' + str(content_array[i])\n",
    "        elif expected_array[i] == 'Entertainment':\n",
    "            entertainment = entertainment + ' ' + str(content_array[i])\n",
    "        elif expected_array[i] == 'Health':\n",
    "            health = health + ' ' + str(content_array[i])\n",
    "        elif expected_array[i] == 'Technology':\n",
    "            technology = technology + ' ' + str(content_array[i])\n",
    "\n",
    "    mask = np.array(Image.open(requests.get('http://www.clker.com/cliparts/3/0/a/1/11971238241310805346noonespillow_Simple_Cloud.svg.med.png',stream=True).raw))\n",
    "\n",
    "    wordcloud1 = WordCloud(stopwords=stop_words, background_color=\"white\", mask=mask).generate(business)\n",
    "    plt.imshow(wordcloud1,interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('q1/business.png')\n",
    "\n",
    "    wordcloud2 = WordCloud(stopwords=stop_words,background_color=\"white\",mask=mask).generate(entertainment)\n",
    "    plt.imshow(wordcloud2,interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('q1/entertainment.png')\n",
    "\n",
    "    wordcloud3 = WordCloud(stopwords=stop_words,background_color=\"white\",mask=mask).generate(health)\n",
    "    plt.imshow(wordcloud3,interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('q1/health.png')\n",
    "\n",
    "    wordcloud4 = WordCloud(stopwords=stop_words,background_color=\"white\",mask=mask).generate(technology)\n",
    "    plt.imshow(wordcloud4,interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('q1/technology.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stpw = get_stop_words('en')\n",
    "stpw.extend(my_additional_stop_words)\n",
    "stpw.extend(['can','also','like'])\n",
    "stpw.extend(['saying', 'said', 'say', 'yes', 'instead', 'meanwhile', 'right', 'really', 'finally', 'now', \n",
    "                       'one', 'suggested', 'says', 'added', 'think', 'know', 'though', 'let', 'going', 'back',\n",
    "                       'well', 'example', 'us', 'yet', 'perhaps', 'actually', 'oh', 'year', 'lastyear',\n",
    "                       'last', 'old', 'first', 'good', 'maybe', 'ask', '.', ',', ':', 'take', 'made', 'n\\'t', 'go', \n",
    "                       'make', 'two', 'got', 'took', 'want', 'much', 'may', 'never', 'second', 'could', 'still', 'get', \n",
    "                       '?', 'would', '(', '\\'', ')', '``', '/', \"''\", '%', '#', '!', 'next', \"'s\", ';', '[', ']', '...',\n",
    "                       'might', \"'m\", \"'d\", 'also', 'something', 'even', 'new', 'lot', 'a', 'thing', 'time', 'way',\n",
    "                       'always', 'whose', 'need', 'people', 'come', 'become', 'another', 'many', 'must', 'too', 'as', 'well'])\n",
    "\n",
    "\n",
    "def wordcloud2():\n",
    "    cat = {}\n",
    "    for i,row in train_data.iterrows():\n",
    "        try:\n",
    "            category = cat[row['Label']]\n",
    "        except KeyError:\n",
    "            cat[row['Label']] = {}\n",
    "            category = cat[row['Label']]\n",
    "        for word in row['Content'].lower().split():\n",
    "            #word = w.lower().split()\n",
    "            if word in stpw:\n",
    "                continue\n",
    "            try:\n",
    "                category[word] += 1\n",
    "            except KeyError:\n",
    "                category[word] = 1\n",
    "        \n",
    "    mask = np.array(Image.open(requests.get('http://www.clker.com/cliparts/3/0/a/1/11971238241310805346noonespillow_Simple_Cloud.svg.med.png',stream=True).raw))\n",
    "\n",
    "    wordcloud5 = WordCloud(stopwords=stop_words, background_color=\"white\", mask=mask)\n",
    "    for categ,wrds in cat.items():\n",
    "        wordcloud5.generate_from_frequencies(wrds)\n",
    "        wordcloud5.to_file('q1/'+categ+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building Hashing vectorizer\n",
      "building tfidf\n",
      "Building SVD\n"
     ]
    }
   ],
   "source": [
    "print(\"building Hashing vectorizer\")\n",
    "hvect = HashingVectorizer(stop_words='english')\n",
    "\n",
    "print(\"building tfidf\")\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english',use_idf=True)\n",
    "#vectorizer = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "\n",
    "print(\"Building SVD\")\n",
    "#truncated svd\n",
    "lsa=TruncatedSVD(n_components = 80)\n",
    "svd_transformer = make_pipeline(vectorizer,lsa)\n",
    "#X_train_svd = svd_transformer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).flatten()\n",
    "y = np.array(y).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before determing model\n",
      "Accuracy =  0.9545999999999999\n",
      "Precision =  0.9532042423661526\n",
      "Recall =  0.9481972591828738\n",
      "f1-score =  0.9505834782722115\n"
     ]
    }
   ],
   "source": [
    "#set candidate parameters\n",
    "#parameters = {'kernel':['rbf'], 'gamma': [0.1]}\n",
    "\n",
    "print(\"before determing model\")\n",
    "\n",
    "#determine the model\n",
    "clf = SVC(kernel='linear',C=8,gamma=0.1)\n",
    "#clf = GridSearchCV(svc,parameters,cv=5)\n",
    "\n",
    "estimators1 = [(\"tf_idf\",vectorizer),(\"svm\",clf)]\n",
    "model1 = Pipeline(estimators1)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "accuracy,precision,recall,f1 = 0,0,0,0\n",
    "\n",
    "for trainI,testI in kf.split(X):\n",
    "    #kfTestCategIds = le.transform(y[testI])\n",
    "    model1.fit(X[trainI],y[trainI])\n",
    "    predictions = model1.predict(X[testI])\n",
    "    \n",
    "    ret = precision_score(y[testI], predictions, average='macro')\n",
    "    precision += ret\n",
    "\n",
    "    ret = recall_score(y[testI], predictions, average='macro')\n",
    "    recall += ret\n",
    "\n",
    "    ret = f1_score(y[testI], predictions, average='macro')\n",
    "    f1 += ret\n",
    "\n",
    "    accuracy += accuracy_score(y[testI], predictions)\n",
    "    \n",
    "#accuracy,precision,recall,f1 = accuracy/float(5),precision/float(5),recall/float(5),f1/float(5)\n",
    "accuracy,precision,recall,f1 = accuracy/5,precision/5,recall/5,f1/5\n",
    "print(\"Accuracy = \",end=' ')\n",
    "print(accuracy)\n",
    "print(\"Precision = \",end=' ')\n",
    "print(precision)\n",
    "print(\"Recall = \",end=' ')\n",
    "print(recall)\n",
    "print(\"f1-score = \",end=' ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9191\n",
      "Precision =  0.9162505739239913\n",
      "Recall =  0.9055515922838586\n",
      "f1-score =  0.9105634725176394\n"
     ]
    }
   ],
   "source": [
    "s = SVC(kernel='linear',C=8,gamma=1)\n",
    "\n",
    "estimators2 = [(\"svd\",svd_transformer),(\"svm\",s)]\n",
    "model2 = Pipeline(estimators2)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "accuracy,precision,recall,f1 = 0,0,0,0\n",
    "\n",
    "for trainI,testI in kf.split(X):\n",
    "    #kfTestCategIds = le.transform(y[testI])\n",
    "    model2.fit(X[trainI],y[trainI])\n",
    "    predictions = model2.predict(X[testI])\n",
    "    \n",
    "    ret = precision_score(y[testI], predictions, average='macro')\n",
    "    precision += ret\n",
    "\n",
    "    ret = recall_score(y[testI], predictions, average='macro')\n",
    "    recall += ret\n",
    "\n",
    "    ret = f1_score(y[testI], predictions, average='macro')\n",
    "    f1 += ret\n",
    "\n",
    "    accuracy += accuracy_score(y[testI], predictions)\n",
    "    \n",
    "#accuracy,precision,recall,f1 = accuracy/float(5),precision/float(5),recall/float(5),f1/float(5)\n",
    "accuracy,precision,recall,f1 = accuracy/5,precision/5,recall/5,f1/5\n",
    "print(\"Accuracy = \",end=' ')\n",
    "print(accuracy)\n",
    "print(\"Precision = \",end=' ')\n",
    "print(precision)\n",
    "print(\"Recall = \",end=' ')\n",
    "print(recall)\n",
    "print(\"f1-score = \",end=' ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests (Bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8968999999999999\n",
      "Precision =  0.9067435816527931\n",
      "Recall =  0.8715337766643311\n",
      "f1-score =  0.8865241827043843\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "estimators3 = [(\"tf_idf\",vectorizer),(\"rfc\",rfc)]\n",
    "model3 = Pipeline(estimators3)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "accuracy,precision,recall,f1 = 0,0,0,0\n",
    "\n",
    "for trainI,testI in kf.split(X):\n",
    "    #kfTestCategIds = le.transform(y[testI])\n",
    "    model3.fit(X[trainI],y[trainI])\n",
    "    predictions = model3.predict(X[testI])\n",
    "    \n",
    "    ret = precision_score(y[testI], predictions, average='macro')\n",
    "    precision += ret\n",
    "\n",
    "    ret = recall_score(y[testI], predictions, average='macro')\n",
    "    recall += ret\n",
    "\n",
    "    ret = f1_score(y[testI], predictions, average='macro')\n",
    "    f1 += ret\n",
    "\n",
    "    accuracy += accuracy_score(y[testI], predictions)\n",
    "    \n",
    "#accuracy,precision,recall,f1 = accuracy/float(5),precision/float(5),recall/float(5),f1/float(5)\n",
    "accuracy,precision,recall,f1 = accuracy/5,precision/5,recall/5,f1/5\n",
    "print(\"Accuracy = \",end=' ')\n",
    "print(accuracy)\n",
    "print(\"Precision = \",end=' ')\n",
    "print(precision)\n",
    "print(\"Recall = \",end=' ')\n",
    "print(recall)\n",
    "print(\"f1-score = \",end=' ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests (svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9264000000000001\n",
      "Precision =  0.9242512323809045\n",
      "Recall =  0.9114466829623454\n",
      "f1-score =  0.9173966039247782\n"
     ]
    }
   ],
   "source": [
    "rfc1 = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "estimators4 = [(\"svd\",svd_transformer),(\"rfc\",rfc1)]\n",
    "model4 = Pipeline(estimators4)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "accuracy,precision,recall,f1 = 0,0,0,0\n",
    "\n",
    "for trainI,testI in kf.split(X):\n",
    "    #kfTestCategIds = le.transform(y[testI])\n",
    "    model4.fit(X[trainI],y[trainI])\n",
    "    predictions = model4.predict(X[testI])\n",
    "    \n",
    "    ret = precision_score(y[testI], predictions, average='macro')\n",
    "    precision += ret\n",
    "\n",
    "    ret = recall_score(y[testI], predictions, average='macro')\n",
    "    recall += ret\n",
    "\n",
    "    ret = f1_score(y[testI], predictions, average='macro')\n",
    "    f1 += ret\n",
    "\n",
    "    accuracy += accuracy_score(y[testI], predictions)\n",
    "    \n",
    "#accuracy,precision,recall,f1 = accuracy/float(5),precision/float(5),recall/float(5),f1/float(5)\n",
    "accuracy,precision,recall,f1 = accuracy/5,precision/5,recall/5,f1/5\n",
    "print(\"Accuracy = \",end=' ')\n",
    "print(accuracy)\n",
    "print(\"Precision = \",end=' ')\n",
    "print(precision)\n",
    "print(\"Recall = \",end=' ')\n",
    "print(recall)\n",
    "print(\"f1-score = \",end=' ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\python\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'actual', 'ad', 'alway', 'ani', 'anoth', 'arent', 'becaus', 'becom', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'exampl', 'final', 'hadnt', 'hasnt', 'havent', 'isnt', 'mani', 'mayb', 'meanwhil', 'mightnt', 'mustnt', 'neednt', 'nt', 'onc', 'onli', 'ourselv', 'peopl', 'perhap', 'realli', 'shant', 'shes', 'shouldnt', 'shouldv', 'someth', 'suggest', 'thatll', 'themselv', 'veri', 'wasnt', 'werent', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9562999999999999\n",
      "Precision =  0.9553819566037911\n",
      "Recall =  0.9497798559735733\n",
      "f1-score =  0.9524677583942159\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopWords = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "stopWords.extend(['saying', 'said', 'say', 'yes', 'instead', 'meanwhile', 'right', 'really', 'finally', 'now', \n",
    "                       'one', 'suggested', 'says', 'added', 'think', 'know', 'though', 'let', 'going', 'back',\n",
    "                       'well', 'example', 'us', 'yet', 'perhaps', 'actually', 'oh', 'year', 'lastyear',\n",
    "                       'last', 'old', 'first', 'good', 'maybe', 'ask', '.', ',', ':', 'take', 'made', 'n\\'t', 'go', \n",
    "                       'make', 'two', 'got', 'took', 'want', 'much', 'may', 'never', 'second', 'could', 'still', 'get', \n",
    "                       '?', 'would', '(', '\\'', ')', '``', '/', \"''\", '%', '#', '!', 'next', \"'s\", ';', '[', ']', '...',\n",
    "                       'might', \"'m\", \"'d\", 'also', 'something', 'even', 'new', 'lot', 'a', 'thing', 'time', 'way',\n",
    "                       'always', 'whose', 'need', 'people', 'come', 'become', 'another', 'many', 'must', 'too', 'as', 'well'])\n",
    "\n",
    "stopWords.extend([' ','are','and','I','A','And','So','arnt','This','When','It','many','Many','so','cant','Yes','yes','No','no','These','these'])\n",
    "\n",
    "def processText(text):    \n",
    "    p_stemmer = SnowballStemmer(language='english')\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    tokens = text.lower()\n",
    "    tokens = tokens.split()\n",
    "    #tokens = [wrd for wrd in tokens if(wrd not in stopWords)]\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    #tokens = [wrd for wrd in tokens if(wrd not in stopWords)]\n",
    "    stems = [p_stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "#X = 'ttt '+ df['Title']+' ttt '+ df['Title']+' ccc '+df['Content']+' ttt '+df['Title']+' ttt '+ df['Title']\n",
    "X = df['Title'] + df['Title'] + df['Content'] + df['Title'] + df['Title']\n",
    "y = df['Label']\n",
    "\n",
    "X = np.array(X).flatten()\n",
    "y = np.array(y).flatten()\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=processText, stop_words = stopWords,use_idf=True)\n",
    "#vectorizer = TfidfVectorizer(tokenizer=processText, use_idf=True)\n",
    "\n",
    "nb = SVC(kernel='rbf',C=8,gamma=0.1)    \n",
    "#nb = KNeighborsClassifier(n_neighbors=3)\n",
    "#nb = ComplementNB()\n",
    "\n",
    "estimators = [(\"tf_idf\",vectorizer),(\"svm\",nb)]\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "accuracy,precision,recall,f1 = 0,0,0,0\n",
    "\n",
    "for trainI,testI in kf.split(X):\n",
    "    #kfTestCategIds = le.transform(y[testI])\n",
    "    model.fit(X[trainI],y[trainI])\n",
    "    predictions = model.predict(X[testI])\n",
    "    \n",
    "    ret = precision_score(y[testI], predictions, average='macro')\n",
    "    precision += ret\n",
    "\n",
    "    ret = recall_score(y[testI], predictions, average='macro')\n",
    "    recall += ret\n",
    "\n",
    "    ret = f1_score(y[testI], predictions, average='macro')\n",
    "    f1 += ret\n",
    "\n",
    "    accuracy += accuracy_score(y[testI], predictions)\n",
    "    \n",
    "#accuracy,precision,recall,f1 = accuracy/float(5),precision/float(5),recall/float(5),f1/float(5)\n",
    "accuracy,precision,recall,f1 = accuracy/5,precision/5,recall/5,f1/5\n",
    "print(\"Accuracy = \",end=' ')\n",
    "print(accuracy)\n",
    "print(\"Precision = \",end=' ')\n",
    "print(precision)\n",
    "print(\"Recall = \",end=' ')\n",
    "print(recall)\n",
    "print(\"f1-score = \",end=' ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = test_data['Title']+test_data['Title']+test_data['Content']+test_data['Title']+test_data['Title']\n",
    "#X_test_vector = vectorizer.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_final = nb.predict(X_test_vector)\n",
    "pred_final = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_print_arr = np.array(test_data[['Id']]).flatten()\n",
    "columns = pd.Index(['Id','Predicted'])\n",
    "data = np.column_stack((to_print_arr,pred_final))\n",
    "df_final = pd.DataFrame(data, index=None, columns = columns)\n",
    "df_final.to_csv('q1/testSet_categories.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement 1 complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
