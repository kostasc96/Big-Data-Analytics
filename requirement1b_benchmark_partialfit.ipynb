{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import urllib3\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem.porter import *\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('datasets/q1/train.csv', sep=',')\n",
    "test_data = pd.read_csv('datasets/q1/test_without_labels.csv', sep=',')\n",
    "\n",
    "\n",
    "my_additional_stop_words = ['said','still','day','will','new','may','two','one','now','time','say','second','month','first','going','year','back','people','case','according']\n",
    "stop_words = STOPWORDS.union(my_additional_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopWords = stopwords.words('english')\n",
    "import re\n",
    "stopWords.extend(['saying', 'said', 'say', 'yes', 'instead', 'meanwhile', 'right', 'really', 'finally', 'now', \n",
    "                       'one', 'suggested', 'says', 'added', 'think', 'know', 'though', 'let', 'going', 'back',\n",
    "                       'well', 'example', 'us', 'yet', 'perhaps', 'actually', 'oh', 'year', 'lastyear',\n",
    "                       'last', 'old', 'first', 'good', 'maybe', 'ask', '.', ',', ':', 'take', 'made', 'n\\'t', 'go', \n",
    "                       'make', 'two', 'got', 'took', 'want', 'much', 'may', 'never', 'second', 'could', 'still', 'get', \n",
    "                       '?', 'would', '(', '\\'', ')', '``', '/', \"''\", '%', '#', '!', 'next', \"'s\", ';', '[', ']', '...',\n",
    "                       'might', \"'m\", \"'d\", 'also', 'something', 'even', 'new', 'lot', 'a', 'thing', 'time', 'way',\n",
    "                       'always', 'whose', 'need', 'people', 'come', 'become', 'another', 'many', 'must', 'too', 'as', 'well'])\n",
    "\n",
    "stopWords.extend([' ','are','and','I','A','And','So','arnt','This','When','It','many','Many','so','cant','Yes','yes','No','no','These','these'])\n",
    "\n",
    "def processText(text):    \n",
    "    p_stemmer = SnowballStemmer(language='english')\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    tokens = text.lower()\n",
    "    tokens = tokens.split()\n",
    "    #tokens = [wrd for wrd in tokens if(wrd not in stopWords)]\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    #tokens = [wrd for wrd in tokens if(wrd not in stopWords)]\n",
    "    stems = [p_stemmer.stem(t) for t in tokens]\n",
    "    stems = [wrd for wrd in stems if(wrd not in stopWords)]\n",
    "    stems = ' '.join(stems)\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building Hashing vectorizer\n",
      "building tfidf\n"
     ]
    }
   ],
   "source": [
    "print(\"building Hashing vectorizer\")\n",
    "hvect = HashingVectorizer()\n",
    "\n",
    "print(\"building tfidf\")\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "#vectorizer = TfidfVectorizer(use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_data['Title'] + train_data['Title'] + train_data['Content'] + train_data['Title'] + train_data['Title']\n",
    "df = df.map(processText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "tfidfv = TfidfTransformer()\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "X = np.array(df).flatten()\n",
    "y = le.fit_transform(train_data['Label'])\n",
    "\n",
    "X = np.array(X).flatten()\n",
    "y = np.array(y).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9663491211592646\n",
      "Precision =  0.9658469974160344\n",
      "Recall =  0.9610289978564847\n",
      "f1-score =  0.9633642354674988\n"
     ]
    }
   ],
   "source": [
    "accuracy,precision,recall,f1 = 0,0,0,0\n",
    "kf = KFold(n_splits=5)\n",
    "from numpy import unique\n",
    "\n",
    "for trainI,testI in kf.split(X):\n",
    "    #kfTestCategIds = le.transform(y[testI])\n",
    "    sgd.partial_fit(tfidfv.fit_transform(hvect.fit_transform(X[trainI])),y[trainI],classes=unique(y))\n",
    "    predictions = sgd.predict(tfidfv.fit_transform(hvect.fit_transform(X[testI])))\n",
    "    \n",
    "    ret = precision_score(y[testI], predictions, average='macro')\n",
    "    precision += ret\n",
    "\n",
    "    ret = recall_score(y[testI], predictions, average='macro')\n",
    "    recall += ret\n",
    "\n",
    "    ret = f1_score(y[testI], predictions, average='macro')\n",
    "    f1 += ret\n",
    "\n",
    "    accuracy += accuracy_score(y[testI], predictions)\n",
    "    \n",
    "#accuracy,precision,recall,f1 = accuracy/float(5),precision/float(5),recall/float(5),f1/float(5)\n",
    "accuracy,precision,recall,f1 = accuracy/5,precision/5,recall/5,f1/5\n",
    "print(\"Accuracy = \",end=' ')\n",
    "print(accuracy)\n",
    "print(\"Precision = \",end=' ')\n",
    "print(precision)\n",
    "print(\"Recall = \",end=' ')\n",
    "print(recall)\n",
    "print(\"f1-score = \",end=' ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = test_data['Title']+test_data['Title']+test_data['Content']+test_data['Title']+test_data['Title']\n",
    "df_test = df_test.map(processText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final = sgd.predict(tfidfv.fit_transform(hvect.fit_transform(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final = le.inverse_transform(pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_print_arr = np.array(test_data[['Id']]).flatten()\n",
    "columns = pd.Index(['Id','Predicted'])\n",
    "data = np.column_stack((to_print_arr,pred_final))\n",
    "df_final = pd.DataFrame(data, index=None, columns = columns)\n",
    "df_final.to_csv('q1/testSet_categories.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
